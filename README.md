## Others
- [Entropy Maximization with Depth: A Variational Principle for Random Neural Networks](https://arxiv.org/pdf/2205.13076.pdf), 2022 arXiv



## Neural Tangent Kernel
- [Order and Chaos: NTK views on DNN Normalization Checkerboard and Boundary Artifacts](https://arxiv.org/pdf/1907.05715.pdf), 2020 arXiv 
- [Mean-field Behaviour of Neural Tangent Kernel for Deep Neural Networks](https://arxiv.org/pdf/1905.13654.pdf), 2021 ICML
- [Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains](https://arxiv.org/pdf/2006.10739.pdf), 2020 NeurIPS. [[CODE](https://github.com/tancik/fourier-feature-networks)]




## Generative Models
- [Diagnosing and Fixing Manifold Overfitting in Deep Generative Models](https://arxiv.org/pdf/2204.07172.pdf), 2022 arXiv. [[CODE](https://github.com/layer6ai-labs/two_step_zoo)]
- 


## FrequencyDNN
- [Theory of the Frequency Principle for General Deep Neural Networks](https://arxiv.org/pdf/1906.09235v2.pdf), 2019 arXiv.
- [A Fourier Perspective on Model Robustness in Computer Vision](https://arxiv.org/pdf/1906.08988.pdf), 2019 NeurIPS. [[CODE](https://github.com/google-research/google-research/tree/master/frequency_analysis)]




## Partial Differential Equations
- [Choose a Transformer: Fourier or Galerkin](https://arxiv.org/pdf/2105.14995.pdf), 2021 NeurIPS. [[CODE](https://github.com/scaomath/galerkin-transformer)]




## Gradient-based Hyperparameter Optimization
- [Optimizing Millions of Hyperparameters by Implicit Differentiation](https://arxiv.org/pdf/1911.02590.pdf), 2020 AISTATS. [[CODE](https://github.com/lorraine2/implicit-hyper-opt)]




## SGD
- [Minibatch vs Local SGD with Shuffling: Tight Convergence Bounds and Beyond](https://arxiv.org/pdf/2110.10342.pdf), 2022 ICLR Oral.



## Transformer
- [Pyraformer: Low-complexity Pyramidal Attention for Long-range Time Series Modeling and Forecasting](https://openreview.net/pdf?id=0EXmFzUn5I), 2022 ICLR Oral.
- [Perceiver: General Perception with Iterative Attention](https://arxiv.org/pdf/2103.03206.pdf), 2021 ICML.



## Thompson Sampling
- [A Tutorial on Thompson Sampling](https://web.stanford.edu/~bvr/pubs/TS_Tutorial.pdf). [[CODE](https://github.com/iosband/ts_tutorial)]

  Bernoulli bandit problems, shortest path problems, product recommendation, assortment, active learning with neural networks, and reinforcement learning in Markov decision processes.
  
- [An Empirical Evaluation of Thompson Sampling](https://papers.nips.cc/paper/2011/file/e53a0a2978c28872a4505bdb51db06dc-Paper.pdf), 2021 NIPS.
- [Feel-Good Thompson Sampling for Contextual Bandits and Reinforcement Learning](https://arxiv.org/pdf/2110.00871.pdf), 2021 arXiv.
- [Improving Regret Bounds for Combinatorial Semi-Bandits with Probabilistically Triggered Arms and Its Applications](https://arxiv.org/pdf/1703.01610.pdf), 2017 NeurIPS. [[CODE]()]
- [Combinatorial Multi-Armed Bandit: GeneralFramework, Results and Applications](http://proceedings.mlr.press/v28/chen13a.pdf), 2013 ICML.
- [求通俗解释下bandit老虎机到底是个什么东西？](https://www.zhihu.com/question/53381093)


## Gradient Sampling
